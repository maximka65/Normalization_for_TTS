{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Устанавливаем нужные зависимости с нужными версиями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XFG0NDRu5mYQ"
   },
   "outputs": [],
   "source": [
    "# !apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
    "# !pip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text\n",
    "# !pip install -q tensorflow_datasets\n",
    "# !pip install -q -U tensorflow-text tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JjJJyJTZYebt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 13:53:22.481117: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Импортируем нужные модули\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import tqdm\n",
    "\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XN2nFtyRTV_K"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 13:53:32.676203: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Загружаем данные взятые из соревнование Text Normalization Challenge - Russian Language\n",
    "# Ссылка на датасет https://www.kaggle.com/competitions/text-normalization-challenge-russian-language/data\n",
    "# Я перевел их в нужный формат;\n",
    "\n",
    "# Тренировочные данные\n",
    "with open('train_list', 'rb') as f:\n",
    "    data_list = pickle.load(f)\n",
    "    \n",
    "x_sentence_list = data_list[0]\n",
    "y_sentence_list = data_list[1]\n",
    "\n",
    "# Тестовые данные которые предоставил преподаватель\n",
    "with open('test/TextTrain.txt', 'r') as f:\n",
    "    test = f.readlines()\n",
    "    \n",
    "with open('test/TextTrue.txt', 'r') as f:\n",
    "    true = f.readlines()\n",
    "\n",
    "clean_test = list(map(lambda x: x.replace('\\n', '').encode('utf-8'), test))\n",
    "clean_true = list(map(lambda x: x.replace('\\n', '').encode('utf-8'), true))\n",
    "\n",
    "# Конвертируем наши листы с предложениями в tf dataset\n",
    "train_tensor = tf.data.Dataset.from_tensor_slices((x_sentence_list, y_sentence_list))\n",
    "val_tensor = tf.data.Dataset.from_tensor_slices((clean_test, clean_true))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "u__F5IVHM7tb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Примеры до нормализации:\n",
      "По состоянию на 1862 год .\n",
      "Оснащались латными рукавицами и сабатонами с не длинными носками .\n",
      "В конце 1811 года , вследствие конфликта с проезжим вельможей ( графом Салтыковым ) вынужден был оставить службу по личному прошению .\n",
      "\n",
      "\t Примеры после нормализации:\n",
      "По состоянию на тысяча восемьсот шестьдесят второй год .\n",
      "Оснащались латными рукавицами и сабатонами с не длинными носками .\n",
      "В конце тысяча восемьсот одиннадцатого года , вследствие конфликта с проезжим вельможей ( графом Салтыковым ) вынужден был оставить службу по личному прошению .\n"
     ]
    }
   ],
   "source": [
    "for before, after in train_tensor.batch(3).take(1):\n",
    "    print('\\t Примеры до нормализации:')\n",
    "    for bf in before.numpy():\n",
    "        print(bf.decode('utf-8'))\n",
    "    print()\n",
    "    print('\\t Примеры после нормализации:')\n",
    "    for aft in after.numpy():\n",
    "        print(aft.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Примеры до нормализации:\n",
      "Было опрошено 187 человек, в т.ч. 91 женщина, 67 мужчин и 29 подростков.\n",
      "В 1660 г. король Людовик XIV женился на инфанте Марии-Терезии Австрийской. \n",
      "Рек длиной более 10 км. насчитывается в области 348, их суммарная длина составляет 10235 км.\n",
      "\n",
      "\t Примеры после нормализации:\n",
      "Было опрошено сто восемьдесят семь человек, в том числе девяносто одна женщина, шестьдесят семь мужчин и двадцать девять подростков.\n",
      "В тысяча шестьсот шестидесятый год король Людовик четырнадцатый женился на инфанте Марии-Терезии Австрийской. \n",
      "Рек длиной более десяти километров насчитывается в области триста сорок восемь, их суммарная длина составляет десять тысяч двести тридцать пять километров\n"
     ]
    }
   ],
   "source": [
    "for before, after in val_tensor.batch(3).take(1):\n",
    "    print('\\t Примеры до нормализации:')\n",
    "    for bf in before.numpy():\n",
    "        print(bf.decode('utf-8'))\n",
    "    print()\n",
    "    print('\\t Примеры после нормализации:')\n",
    "    for aft in after.numpy():\n",
    "        print(aft.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zaZBpTRjD-vj"
   },
   "outputs": [],
   "source": [
    "# Define a tokenizer class\n",
    "import pathlib\n",
    "import re\n",
    "\n",
    "bert_tokenizer_params = dict(lower_case=True)\n",
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
    "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
    "\n",
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "    # Remove reserved tokens except for \"[UNK]\".\n",
    "    bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
    "    bad_token_re = \"|\".join(bad_tokens)\n",
    "\n",
    "    bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
    "    result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "    # Join the tokens into strings.\n",
    "    result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
    "\n",
    "    return result\n",
    "\n",
    "def add_start_end(ragged):\n",
    "    count = ragged.bounding_shape()[0]\n",
    "    starts = tf.fill([count,1], START)\n",
    "    ends = tf.fill([count,1], END)\n",
    "    return tf.concat([starts, ragged, ends], axis=1)\n",
    "\n",
    "class CustomTokenizer(tf.Module):\n",
    "    def __init__(self, reserved_tokens, vocab_path):\n",
    "        self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
    "        self._reserved_tokens = reserved_tokens\n",
    "        self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "\n",
    "        vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
    "        self.vocab = tf.Variable(vocab)\n",
    "\n",
    "        ## Create signatures for export:   \n",
    "\n",
    "        # Include a tokenize signature for a batch of strings. \n",
    "        self.tokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None], dtype=tf.string))\n",
    "\n",
    "        # Include `detokenize` and `lookup` signatures for:\n",
    "        #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
    "        #   * `RaggedTensors` with shape [batch, tokens]\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "        self.lookup.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        self.lookup.get_concrete_function(\n",
    "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "        # These `get_*` methods take no arguments\n",
    "        self.get_vocab_size.get_concrete_function()\n",
    "        self.get_vocab_path.get_concrete_function()\n",
    "        self.get_reserved_tokens.get_concrete_function()\n",
    "\n",
    "    @tf.function\n",
    "    def tokenize(self, strings):\n",
    "        enc = self.tokenizer.tokenize(strings)\n",
    "        # Merge the `word` and `word-piece` axes.\n",
    "        enc = enc.merge_dims(-2, -1)\n",
    "        enc = add_start_end(enc)\n",
    "        return enc\n",
    "\n",
    "    @tf.function\n",
    "    def detokenize(self, tokenized):\n",
    "        words = self.tokenizer.detokenize(tokenized)\n",
    "        return cleanup_text(self._reserved_tokens, words)\n",
    "\n",
    "    @tf.function\n",
    "    def lookup(self, token_ids):\n",
    "        return tf.gather(self.vocab, token_ids)\n",
    "\n",
    "    @tf.function\n",
    "    def get_vocab_size(self):\n",
    "        return tf.shape(self.vocab)[0]\n",
    "\n",
    "    @tf.function\n",
    "    def get_vocab_path(self):\n",
    "        return self._vocab_path\n",
    "\n",
    "    @tf.function\n",
    "    def get_reserved_tokens(self):\n",
    "        return tf.constant(self._reserved_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9VqEDpYSEKGl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ted_hrlr_translate_ru_en_converter/assets\n"
     ]
    }
   ],
   "source": [
    "# Создаем токенизатор на основе слов на русском собранных из TED Talks\n",
    "tokenizers = tf.Module()\n",
    "tokenizers.ru = CustomTokenizer(reserved_tokens, 'ru_vocab.txt')\n",
    "\n",
    "model_name = 'ted_hrlr_translate_ru_en_converter'\n",
    "tf.saved_model.save(tokenizers, model_name);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Yb35sTJcZq9"
   },
   "source": [
    "### Создадим пайплайн данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Dggbe-yKRdRo",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define a function to prepare a batch of inputs and labels\n",
    "def preprocess_batch(source, target):\n",
    "    # Tokenize the source text\n",
    "    source_tokens = tokenizers.ru.tokenize(source)\n",
    "    source_tokens = source_tokens[:, :128]\n",
    "    source_tensor = source_tokens.to_tensor()\n",
    "\n",
    "    # Tokenize the target text\n",
    "    target_tokens = tokenizers.ru.tokenize(target)\n",
    "    target_tokens = target_tokens[:, :(128 + 1)]\n",
    "\n",
    "    # Prepare the inputs and labels for the decoder\n",
    "    target_inputs = target_tokens[:, :-1].to_tensor()\n",
    "    target_labels = target_tokens[:, 1:].to_tensor()\n",
    "\n",
    "    return (source_tensor, target_inputs), target_labels\n",
    "\n",
    "def make_batches(ds):\n",
    "    return (\n",
    "        ds.shuffle(20000)\n",
    "          .batch(64)\n",
    "          .map(preprocess_batch, tf.data.AUTOTUNE)\n",
    "          .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "\n",
    "# Create training and validation set batches.\n",
    "train_batches = make_batches(train_tensor)\n",
    "val_batches = make_batches(val_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "838tmM1cm9cB"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth/2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]     \n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth\n",
    "  \n",
    "    angle_rates = 1 / (10000**depths)\n",
    "    angle_rads = positions * angle_rates\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "      axis=-1) \n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x\n",
    "\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context):\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=context,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "   \n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            use_causal_mask = True)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(dff, activation='relu'),\n",
    "          tf.keras.layers.Dense(d_model),\n",
    "          tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x) \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ncyS-Ms3i2x_"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(\n",
    "            vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "        self.self_attention_layers = [\n",
    "            GlobalSelfAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "        self.ffn_layers = [\n",
    "            FeedForward(d_model, dff)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.pos_embedding(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.self_attention_layers[i](x)\n",
    "            x = self.ffn_layers[i](x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "               dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                                 d_model=d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.causal_self_attention_layers = [\n",
    "            CausalSelfAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "        self.cross_attention_layers = [\n",
    "            CrossAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "        self.ffn_layers = [\n",
    "            FeedForward(d_model, dff)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x, context):\n",
    "    \n",
    "        x = self.pos_embedding(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.causal_self_attention_layers[i](x)\n",
    "            x = self.cross_attention_layers[i](x, context)\n",
    "            self.last_attn_scores = self.cross_attention_layers[i].last_attn_scores\n",
    "            x = self.ffn_layers[i](x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               vocab_size=input_vocab_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               vocab_size=target_vocab_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        context, x  = inputs\n",
    "        context = self.encoder(context) \n",
    "        x = self.decoder(x, context)  \n",
    "        logits = self.final_layer(x)\n",
    "        \n",
    "        try:\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "nCdBGtuxH8cV"
   },
   "outputs": [],
   "source": [
    "# Зададим гиперпараметры\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "\n",
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=tokenizers.ru.get_vocab_size().numpy(),\n",
    "    target_vocab_size=tokenizers.ru.get_vocab_size().numpy(),\n",
    "    dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dZXT2IYxIF8k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 86)\n",
      "(64, 128)\n",
      "(64, 128, 7832)\n"
     ]
    }
   ],
   "source": [
    "for (bef, aft), aft_labels in train_batches.take(1):\n",
    "    break\n",
    "\n",
    "output = transformer((bef, aft))\n",
    "\n",
    "print(bef.shape)\n",
    "print(aft.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "IsUPhlfEtOjn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder (Encoder)           multiple                  3641344   \n",
      "                                                                 \n",
      " decoder (Decoder)           multiple                  5752320   \n",
      "                                                                 \n",
      " dense_16 (Dense)            multiple                  1010328   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,403,992\n",
      "Trainable params: 10,403,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# выведем структуру сети\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfoBfC2oQtEy"
   },
   "source": [
    "## Тренировка сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "iYQdOO1axwEI"
   },
   "outputs": [],
   "source": [
    "# Define a loss function that computes the masked sparse categorical cross-entropy loss\n",
    "def masked_loss(labels, predictions):\n",
    "    mask = labels != 0\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    losses = loss_fn(labels, predictions)\n",
    "    mask = tf.cast(mask, dtype=losses.dtype)\n",
    "    losses *= mask\n",
    "    loss = tf.reduce_sum(losses) / tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "# Define a function that computes the masked accuracy\n",
    "def masked_accuracy(labels, predictions):\n",
    "    predicted_labels = tf.argmax(predictions, axis=2)\n",
    "    labels = tf.cast(labels, predicted_labels.dtype)\n",
    "    correct_matches = labels == predicted_labels\n",
    "    mask = labels != 0\n",
    "    correct_matches = correct_matches & mask\n",
    "    correct_matches = tf.cast(correct_matches, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_sum(correct_matches) / tf.reduce_sum(mask)\n",
    "    return accuracy\n",
    "\n",
    "# Define a custom learning rate schedule for the Adam optimizer\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        lr = tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "        return lr\n",
    "    \n",
    "# Create an instance of the CustomSchedule class and use it to create an Adam optimizer\n",
    "learning_rate_schedule = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "transformer.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])\n",
    "\n",
    "transformer.fit(train_batches,\n",
    "                epochs=20,\n",
    "                validation_data=val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "eY_uXsOhSmbb"
   },
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "    \"\"\"Class for getting network predictions\"\"\"\n",
    "    def __init__(self, tokenizers, transformer):\n",
    "        self.tokenizers = tokenizers\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __call__(self, sentence, max_length=128):\n",
    "        assert isinstance(sentence, tf.Tensor)\n",
    "        if len(sentence.shape) == 0:\n",
    "            sentence = sentence[tf.newaxis]\n",
    "\n",
    "        sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n",
    "\n",
    "        encoder_input = sentence\n",
    "\n",
    "        start_end = self.tokenizers.en.tokenize([''])[0]\n",
    "        start = start_end[0][tf.newaxis]\n",
    "        end = start_end[1][tf.newaxis]\n",
    "\n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "        output_array = output_array.write(0, start)\n",
    "\n",
    "        for i in range(max_length):\n",
    "            output = tf.transpose(output_array.stack())\n",
    "            predictions = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "            predictions = predictions[:, -1:, :]\n",
    "\n",
    "            predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "            output_array = output_array.write(i+1, predicted_id[0])\n",
    "\n",
    "            if predicted_id == end:\n",
    "                break\n",
    "\n",
    "        output = tf.transpose(output_array.stack())\n",
    "        text = self.tokenizers.ru.detokenize(output)[0]\n",
    "\n",
    "        tokens = self.tokenizers.ru.lookup(output)[0]\n",
    "        self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "        attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "        return text, tokens, attention_weights\n",
    "\n",
    "    \n",
    "def calculate_bleu_scores(translator, sentences, references):\n",
    "    \"\"\"\n",
    "    Calculates BLEU score for each sentence translated by the given translator \n",
    "    against its corresponding reference sentence. Returns the average BLEU score.\n",
    "    \"\"\"\n",
    "    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "    \n",
    "    clean_references = [r.replace('\\n', '').split(' ') for r in references]\n",
    "    preds = []\n",
    "    sum_bleu = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
    "        preds.append(translated_text)\n",
    "        \n",
    "    for i in range(len(clean_references)):\n",
    "        bleu = sentence_bleu(clean_references[i], preds[i], smoothing_function=SmoothingFunction(epsilon=1).method7)\n",
    "        sum_bleu += bleu\n",
    "    \n",
    "    return sum_bleu / len(clean_references)\n",
    "\n",
    "\n",
    "with open('test/TextTrue.txt', 'r') as f:\n",
    "    true = f.readlines()\n",
    "\n",
    "with open('TextTrain.txt', 'r') as f:\n",
    "    train = f.readlines()\n",
    "\n",
    "with open('TextTest.txt', 'r') as f:\n",
    "    test = f.readlines()\n",
    "\n",
    "clean_train = [t.replace('\\n', '') for t in train]\n",
    "clean_test = [t.replace('\\n', '') for t in test]\n",
    "\n",
    "translator = Translator(tokenizers, transformer)\n",
    "\n",
    "bleu_train = calculate_bleu_scores(translator, clean_train, true)\n",
    "bleu_test = calculate_bleu_scores(translator, clean_test, true)\n",
    "\n",
    "print('BLEU for stt_normalizer: ', bleu_test)\n",
    "print('BLEU for non normalized text: ', bleu_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zz4uIDbT1OU"
   },
   "source": [
    "## Экспорт модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "NZhv5h4AT_n5"
   },
   "outputs": [],
   "source": [
    "class ExportTranslator(tf.Module):\n",
    "    def __init__(self, translator):\n",
    "        self.translator = translator\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
    "    def __call__(self, sentence):\n",
    "        (result,\n",
    "         tokens,\n",
    "         attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)\n",
    "\n",
    "        return result\n",
    "\n",
    "translator = ExportTranslator(translator)\n",
    "tf.saved_model.save(translator, export_dir='translator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = tf.saved_model.load('translator')\n",
    "reloaded('185 г. до Христа').numpy()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
